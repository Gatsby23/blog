---
title: Machine Learning
date: 2016-04-25
keywords:
tags:
    - machine learning
    - deep learning
...

Machine Learning
================

<i class="icon-book"></i>
<i class="icon-cloud-download"></i>
<i class="icon-exclamation-sign"></i>
<i class="icon-flag"></i>
<i class="icon-heart"></i>
<i class="icon-heart-empty"></i>
<i class="icon-info-sign"></i>
<i class="icon-lightbulb"></i>
<i class="icon-pushpin"></i>
<i class="icon-star"></i>
<i class="icon-star-empty"></i>
<i class="icon-tag"></i>
<i class="icon-tags"></i>
<i class="icon-thumbs-down"></i>
<i class="icon-thumbs-up"></i>

```
ml
├── DeepLearning
│   ├── Bag-of-Words models.pdf
│   ├── Beyond Bags of Features Spatial Pyramid Matching-CVPR2006.pdf
│   ├── Deep Learning - Methods and Applications.pdf
│   ├── Fast accurate detection of 100000 object classes on a single machine-CVPR2013.pdf
│   ├── faster r-cnn towards real-time object detection with region proposal networks-NIPS2015.pdf
│   ├── Fast R-CNN-ICCV2015.pdf
│   ├── ImageNet Classification with Deep Convolutional Neural Networks-NIPS2012.pdf
│   ├── LabelMe a database and web-based tool for image.pdf
│   ├── Microsoft COCO Common Objects in Context-ECCV2014.pdf
│   ├── Object Detection with Discriminatively Trained Part Based Model-ppt.pdf
│   ├── Object Detection with Discriminatively Trained Part-Based Models-PAMI2014.pdf
│   ├── R-CNN for object detection-ppt.pdf
│   ├── Recognition using Regions-CVPR2009.pdf
│   ├── Regionlets for Generic Object Detection-ICCV2013.pdf
│   ├── Rich feature hierarchies for accurate object detection and semantic segmentation-CVPR2013.pdf
│   ├── Selective Search for Object Recognition-IJCV2013.pdf
│   ├── Semantic Segmentation with Second-Order Pooling.pdf
│   ├── Spatial Pyramid Pooling in Deep Convolutional Networks for Visual Recognition-PAMI2015.pdf
│   ├── What is an object-CVPR2010.pdf
│   ├── 博客Bag-of-words.pdf
│   ├── 博客SPM.pdf
│   ├── 如何评价rcnn、fast-rcnn和faster-rcnn这一系列方法？ - 机器学习 - 知乎.pdf
│   ├── 深度学习研究理解：SSP.pdf
│   ├── 论文笔记Fast R-CNN.pdf
│   ├── 论文笔记SPP-net.pdf
│   └── 论文笔记SPP.pdf
├── EdgeContour
│   ├── CVPR2015-DeepContour-A-Deep-Convolutional-Feature-Learned-by-Positive-sharing-Loss-for-Contour-Detection-draft-version.pdf
│   ├── CVPR2015-DeepEdge-A-Multi-Scale-Bifurcated-Deep-Network-for-Top-Down-Contour-Detection.pdf
│   ├── DeepContour - A Deep Convolutional Feature Learned by Positive-sharing Lossfor Contour Detection.pdf
│   ├── DeepEdge - A Multi-Scale Bifurcated Deep Networkfor Top-Down Contour Detection.pdf
│   └── EdgeLineDetection.pdf
└── TextDetection
    ├── 1510.03283v1.pdf
    ├── 5638-faster-r-cnn-towards-real-time-object-detection-with-region-proposal-networks.pdf
    ├── Automatic Script Identification in the Wild.pdf
    ├── Detecting Texts of Arbitrary Orientations in Natural Images.pdf
    ├── FCS_TextSurvey_2015.pdf
    ├── Gordo_Supervised_Mid-Level_Features_2015_CVPR_paper.pdf
    ├── Text Flow - A Unified Text Detection System in Natural Scene Images.pdf
    ├── wangwucoatesng_icpr2012.pdf
    └── Yao_Strokelets_2014_CVPR_paper.pdf

3 directories, 40 files

- 1510.03283v1.pdf
- 5638-faster-r-cnn-towards-real-time-object-detection-with-region-proposal-networks.pdf
- Automatic Script Identification in the Wild.pdf
- Bag-of-Words models.pdf
- Beyond Bags of Features Spatial Pyramid Matching-CVPR2006.pdf
- CVPR2015-DeepContour-A-Deep-Convolutional-Feature-Learned-by-Positive-sharing-Loss-for-Contour-Detection-draft-version.pdf
- CVPR2015-DeepEdge-A-Multi-Scale-Bifurcated-Deep-Network-for-Top-Down-Contour-Detection.pdf
- Deep Learning - Methods and Applications.pdf
- DeepContour - A Deep Convolutional Feature Learned by Positive-sharing Lossfor Contour Detection.pdf
- DeepEdge - A Multi-Scale Bifurcated Deep Networkfor Top-Down Contour Detection.pdf
- Detecting Texts of Arbitrary Orientations in Natural Images.pdf
- EdgeLineDetection.pdf
- FCS_TextSurvey_2015.pdf
- Fast R-CNN-ICCV2015.pdf
- Fast accurate detection of 100000 object classes on a single machine-CVPR2013.pdf
- Gordo_Supervised_Mid-Level_Features_2015_CVPR_paper.pdf
- ImageNet Classification with Deep Convolutional Neural Networks-NIPS2012.pdf
- LabelMe a database and web-based tool for image.pdf
- Microsoft COCO Common Objects in Context-ECCV2014.pdf
- Object Detection with Discriminatively Trained Part Based Model-ppt.pdf
- Object Detection with Discriminatively Trained Part-Based Models-PAMI2014.pdf
- R-CNN for object detection-ppt.pdf
- Recognition using Regions-CVPR2009.pdf
- Regionlets for Generic Object Detection-ICCV2013.pdf
- Rich feature hierarchies for accurate object detection and semantic segmentation-CVPR2013.pdf
- Selective Search for Object Recognition-IJCV2013.pdf
- Semantic Segmentation with Second-Order Pooling.pdf
- Spatial Pyramid Pooling in Deep Convolutional Networks for Visual Recognition-PAMI2015.pdf
- Text Flow - A Unified Text Detection System in Natural Scene Images.pdf
- What is an object-CVPR2010.pdf
- Yao_Strokelets_2014_CVPR_paper.pdf
- faster r-cnn towards real-time object detection with region proposal networks-NIPS2015.pdf
- wangwucoatesng_icpr2012.pdf
- 博客Bag-of-words.pdf
- 博客SPM.pdf
- 如何评价rcnn、fast-rcnn和faster-rcnn这一系列方法？ - 机器学习 - 知乎.pdf
- 深度学习研究理解：SSP.pdf
- 论文笔记Fast R-CNN.pdf
- 论文笔记SPP-net.pdf
- 论文笔记SPP.pdf
```

二：课程资源

1. Tom Mitchell：http://work.caltech.edu/library/181.html http://www.cs.cmu.edu/~tom/10701_sp11/lectures.shtml
2. Andrew Ng：https://www.coursera.org/learn/machine-learning/home/welcome
3. NewYork University：http://cs.nyu.edu/~dsontag/courses/ml14/
4. Stanford CS231:http://vision.stanford.edu/teaching/cs231n/index.html
5. Youshua Bengio：http://deeplearning.net/tutorial/他编写的书《Deep Learning》：http://deeplearning.net/tutorial/contents.html
6. Andrew Stanford课程：UFLDL：http://ufldl.stanford.edu/tutorial/

三、代码资源：

1. Keras：https://github.com/fchollet/keras Keras Documentation：http://keras.io/
2. Scikit-Learn: Machine Learning in Python: http://scikit-learn.org/stable/
3. PredictionIO: Open Source Machine Learning Server: https://prediction.io/
4. Visual Recognition and Search(计算机视觉资源汇总网站): http://rogerioferis.com/VisualRecognitionAndSearch2014/Resources.html
5. JMLR Machine Learning Open Source Software: http://jmlr.org/mloss/

### (res) 深度学习研究理解：SSP.pdf

refs and see also

  - [深度学习研究理解7：Spatial Pyramid Pooling in Deep Convolutional Networks for Visual Recognition - whiteinblue的专栏 - 博客频道 - CSDN.NET](http://blog.csdn.net/whiteinblue/article/details/43415035)
  - [Spatial Pyramid Pooling in Deep Convolutional Networks for Visual Recognition - caoeryingzi - 博客园](http://www.cnblogs.com/jianyingzhou/p/3975075.html)
  - [深度学习入门必看的书和论文？有哪些必备的技能需学习？ - 深度学习（Deep Learning） - 知乎](http://www.zhihu.com/question/31785984)
  - [Reading List « Deep Learning](http://deeplearning.net/reading-list/)

[如何评价rcnn、fast-rcnn和faster-rcnn这一系列方法？ - 知乎](https://www.zhihu.com/question/35887527)

:   (res) 如何评价rcnn、fast-rcnn和faster-rcnn这一系列方法？ - 机器学习 - 知乎.pdf

    提到这两个工作，不得不提到RBG大神rbg's home page，该大神在读博士的时候就因
    为dpm获得过pascal voc的终身成就奖。博士后期间更是不断发力，RCNN和Fast- RCNN
    就是他的典型作品。

    RCNN：RCNN可以看作是RegionProposal+CNN这一框架的开山之作，在
    imgenet/voc/mscoco上基本上所有top的方法都是这个框架，可见其影响之大。RCNN的
    主要缺点是重复计算，后来MSRA的kaiming组的SPPNET做了相应的加速。

    Fast-RCNN：RCNN的加速版本，在我看来，这不仅仅是一个加速版本，其优点还包括：

      - 首先，它提供了在caffe的框架下，如何定义自己的层/参数/结构的范例，这个范
        例的一个重要的应用是python layer的应用，我在这里支持多label的caffe，有
        比较好的实现吗？ - 孔涛的回答也提到了。
      - training and testing end-to-end 这一点很重要，为了达到这一点其定义了
        ROIPooling层，因为有了这个，使得训练效果提升不少。
      - 速度上的提升，因为有了Fast-RCNN，这种基于CNN

    的 real-time 的目标检测方法看到了希望，在工程上的实践也有了可能，后续也出现
    了诸如Faster-RCNN/YOLO等相关工作。

    这个领域的脉络是：RCNN -> SPPNET -> Fast-RCNN -> Faster-RCNN。关于具体的细
    节，建议题主还是阅读相关文献吧。

    这使我看到了目标检测领域的希望。起码有这么一部分人，他们不仅仅是为了几个百
    分点的提升，而是切实踏实在做贡献，相信不久这个领域会有新的工作出来。以上纯
    属个人观点，欢迎批评指正。

    是这样的，如果都用一句话来描述

    RCNN 解决的是，“为什么不用CNN做classification呢？”
    （但是这个方法相当于过一遍network出bounding box，再过另一个出label，原文写的很不“elegant”

    Fast-RCNN 解决的是，“为什么不一起输出bounding box和label呢？”
    （但是这个时候用selective search generate regional proposal的时间实在太长了

    Faster-RCNN 解决的是，“为什么还要用selective search呢？”

    于是就达到了real-time。开山之作确实是开山之作，但是也是顺应了“Deep learning 搞一切vision”这一潮流吧。

    refs and see also

      - [rbg's home page](http://www.cs.berkeley.edu/~rbg/index.html)
      - [rbgirshick (Ross Girshick)](https://github.com/rbgirshick)

[如何简单形象又有趣地讲解神经网络是什么？ - 知乎](https://www.zhihu.com/question/22553761)

:   2012年多伦多大学的Krizhevsky等人构造了一个超大型卷积神经网络[1]，有9层，共
    65万个神经元，6千万个参数。网络的输入是图片，输出是1000个类，比如小虫、美洲
    豹、救生船等等。这个模型的训练需要海量图片，它的分类准确率也完爆先前所有分
    类器。纽约大学的Zeiler和Fergusi[2]把这个网络中某些神经元挑出来，把在其上响
    应特别大的那些输入图像放在一起，看它们有什么共同点。他们发现中间层的神经元
    响应了某些十分抽象的特征。

      - 第一层神经元主要负责识别颜色和简单纹理
      - 第二层的一些神经元可以识别更加细化的纹理，比如布纹、刻度、叶纹。
      - 第三层的一些神经元负责感受黑夜里的黄色烛光、鸡蛋黄、高光。
      - 第四层的一些神经元负责识别萌狗的脸、七星瓢虫和一堆圆形物体的存在。
      - 第五层的一些神经元可以识别出花、圆形屋顶、键盘、鸟、黑眼圈动物。

[如何向非物理专业的同学解释重整化群？ - 知乎](https://www.zhihu.com/question/30174067)

:   怎么办呢？你想了想，觉得铁球这么大，你不用把模拟搞得这么精细也能得到正确答
    案。所以你决定把模拟用的水分子体积加10倍，这样就只要模拟10^25个分子了。但是
    光这样搞不行，得出的结果肯定不对，因为有些纳米级的小运动造成的宏观效果没了
    。这时你有一个学生说，老板，其实咱可以试着改改另外那4000个参数，说不定能把
    失去的东西给补偿回来。你觉得靠谱，开动聪明的大脑想了想，心算出了每个参数需
    要的改变。于是你用更大的分子和新的参数重新计算，精确的再现了之前得到的数据
    。（注意，这时你已经对你的系统进行了一次 renormalization）

    在你的nature 文章里，把为了简化计算发明的这个方法叫Renormalization group
    (RG)。把每次模拟时水分子的大小叫做RG scale, 然后你把每次用的参数按照水分子
    的大小列了个表，把它们在尺寸增加时的变化，叫做参数的RG running。你预见到场
    论里的应用，把用这种方法得到的这个新模型，叫做low energy effective field
    theory （EFT）.

    最后，你有点惊讶的发现，当你一步步增大水分子尺寸时，本来都很关键的4000个参
    数，有些干脆变成0了，有些参数和其它的参数成正比了。总之到最后，你只用了大概
    10个自由参数就完美的描述了这一杯水。你把那些最后没用的参数叫irrelevant
    parameters，把它们描述的形状/作用力叫irrelevant operator. 你把这些
    irrelevant parameter/operator 都去掉，得到的那个精简的理论模型就叫做
    renormalizable theory。它和你之前得到的EFT几乎是一样的。

    吐个槽，“重整化群”真是物理名词界的一朵奇葩，把一个本来平易近人的词翻译的不
    明觉厉。这个词英文是 renormalization group（RG）. Normalize 大家都认得，基
    本意思是给一个变量乘个常数，让它更符合一些简单要求。比如几何里说 normalized
    vector, 就是说改变了一个矢量的定义，让它的长度等于一. re-normalize 就是不断
    的 normalize. group 这里是泛指变换，不指数学上严格的群。renormalization
    group 的字面意思就是“不断重新定义参数的一组变换”。

    **重整化群有效本质原因是不同尺度的过程之间往往有一种相对的独立性。**（应该
    说这种问题源自不同尺度之间的独立性，重整化群来自它们之间的共通性）
    如果你的系统是这样的，那重整化群的方法会给你有用的结果。

    refs and see also

      - [如何理解「深度学习和重整化群可以建立严格映射」？这一结论对领域有何影响？ - 知乎](https://www.zhihu.com/question/29854624)

[如何看待 2014 年以来计算机视觉（Computer Vision）界创业潮？ - 知乎](https://www.zhihu.com/question/31430100)

:   首先，毋庸置疑，computer vision作为一个研究领域，正处在整个人工智能史上发展速度最惊人的阶段

    从research的角度来看，这是vision最好的一个时代，也是最坏的一个时代。

    利益相关：我在Cogtu，女朋友在Linkface

[CNN(卷积神经网络)、RNN(循环神经网络)、DNN(深度神经网络)的内部网络结构有什么区别？ - 知乎](https://www.zhihu.com/question/34681168)

:   神经网络技术起源于上世纪五、六十年代，当时叫感知机（perceptron），拥有输入
    层、输出层和一个隐含层。输入的特征向量通过隐含层变换达到输出层，在输出层得
    到分类结果。早期感知机的推动者是Rosenblatt。

    随着数学的发展，这个缺点直到上世纪八十年代才被Rumelhart、Williams、Hinton、
    LeCun等人（反正就是一票大牛）发明的多层感知机（multilayer perceptron）克服
    。

    多层感知机可以摆脱早期离散传输函数的束缚，使用sigmoid或tanh等连续函数模拟神
    经元对激励的响应，在训练算法上则使用Werbos发明的反向传播BP算法。对，这货就
    是我们现在所说的神经网络NN——神经网络听起来不知道比感知机高端到哪里去了！这
    再次告诉我们起一个好听的名字对于研（zhuang）究（bi）很重要！

    多层感知机给我们带来的启示是，神经网络的层数直接决定了它对现实的刻画能力——
    利用每层更少的神经元拟合更加复杂的函数[1]。（Bengio如是说：functions that
    can be compactly represented by a depth k architecture might require an
    exponential number of computational elements to be represented by a depth k
    − 1 architecture.）

    随着神经网络层数的加深，优化函数越来越容易陷入局部最优解

    <span title="具体来说，我们常常使用sigmoid作为神经元的输入输出函数。对于幅度为1的信号，在BP反向传播梯度时，每传递一层，梯度衰减为原来的0.25。层数一多，梯度指数衰减后低层基本上接受不到有效的训练信号。">“梯度消失”现象更加严重。</span>

    2006年，Hinton利用预训练方法缓解了局部最优解问题，将隐含层推动到了7层，
    神经网络真正意义上有了“深度”，由此揭开了深度学习的热潮。这里的“深度”并没有
    固定的定义——在语音识别中4层网络就能够被认为是“较深的”，而在图像识别中20层以
    上的网络屡见不鲜。为了克服梯度消失，ReLU、maxout等传输函数代替了sigmoid，形
    成了如今DNN的基本形式。单从结构上来说，全连接的DNN和图1的多层感知机是没有任
    何区别的

    - 高速公路网络（highway network）和
    - 深度残差学习（deep residual learning）

    全连接DNN的结构里下层神经元和所有上层神经元都能够形成连接，带来的潜在问题是
    参数数量的膨胀。

    事实上，不论是那种网络，他们在实际应用中常常都混合着使用，比如CNN和RNN在上
    层输出之前往往会接上全连接层，很难说某个网络到底属于哪个类别。不难想象随着
    深度学习热度的延续，更灵活的组合方式、更多的网络结构将被发展出来。尽管看起
    来千变万化，但研究者们的出发点肯定都是为了解决特定的问题。题主如果想进行这
    方面的研究，不妨仔细分析一下这些结构各自的特点以及它们达成目标的手段。

    入门的话可以参考：

      - Ng写的Ufldl：UFLDL教程 - Ufldl, 也可以看
      - Theano 内自带的教程，例子非常具体：Deep Learning Tutorials

    1、[Unsupervised Feature Learning and Deep Learning Tutorial](http://deeplearning.stanford.edu/tutorial/)

    :   主页：[Deep Learning](http://ufldl.stanford.edu/)

          - [UFLDL教程 - Ufldl](http://deeplearning.stanford.edu/wiki/index.php/UFLDL%E6%95%99%E7%A8%8B)
          - [UFLDL Tutorial - Ufldl](http://deeplearning.stanford.edu/wiki/index.php/UFLDL_Tutorial)

        这是我最开始接触神经网络时看的资料，把这个仔细研究完会对神经网络的模型
        以及如何训练（反向传播算法）有一个基本的认识，算是一个基本功。

        <p><i class="icon-camera-retro icon-large"></i> icon-camera-retro</p>

    2、[Deep Learning Tutorials — DeepLearning 0.1 documentation](http://deeplearning.net/tutorial/)

    :   这是一个开源的深度学习工具包，里面有很多深度学习模型的python代码还有一
        些对模型以及代码细节的解释。我觉得学习深度学习光了解模型是不难的，难点
        在于把模型落地写成代码，因为里面会有很多细节只有动手写了代码才会了解。
        但Theano也有缺点，就是极其难以调试，以至于我后来就算自己动手写几百行的
        代码也不愿意再用它的工具包。所以我觉得Theano的正确用法还是在于看里面解
        释的文字，不要害怕英文，这是必经之路。PS：推荐使用python语言，目前来看
        比较主流。（更新：自己写坑实在太多了，CUDA也不知道怎么用，还是乖乖用
        theano吧...）

    3、[Stanford University CS231n: Convolutional Neural Networks for Visual Recognition](http://vision.stanford.edu/teaching/cs231n/)

    :   斯坦福的一门课：卷积神经网络，李飞飞教授主讲。这门课会系统的讲一下卷积
        神经网络的模型，然后还有一些课后习题，题目很有代表性，也是用python写的
        ，是在一份代码中填写一部分缺失的代码。如果把这个完整学完，相信使用卷积
        神经网络就不是一个大问题了。

    4、[斯坦福大学公开课 ：机器学习课程_全20集_网易公开课](http://open.163.com/special/opencourse/machinelearning.html)

    :   这可能是机器学习领域最经典最知名的公开课了，由大牛Andrew Ng主讲，这个就
        不仅仅是深度学习了，它是带你领略机器学习领域中最重要的概念，然后建立起
        一个框架，使你对机器学习这个学科有一个较为完整的认识。这个我觉得所有学
        习机器学习的人都应该看一下，我甚至在某公司的招聘要求上看到过：认真看过
        并深入研究过Andrew Ng的机器学习课程，由此可见其重要性。

[有哪些LSTM(Long Short Term Memory)和RNN(Recurrent)网络的教程？ - 知乎](https://www.zhihu.com/question/29411132)

:   先给出一个最快的了解+上手的教程：

    直接看theano官网的LSTM教程+代码：[LSTM Networks for Sentiment Analysis — DeepLearning 0.1 documentation](http://deeplearning.net/tutorial/lstm.html)
    但是，前提是你有RNN的基础，因为LSTM本身不是一个完整的模型，LSTM是对RNN隐含
    层的改进。一般所称的LSTM网络全叫全了应该是使用LSTM单元的RNN网络。教程就给了
    个LSTM的图，它只是RNN框架中的一部分，如果你不知道RNN估计看不懂。比较好的是
    ，你只需要了解前馈过程，你都不需要自己求导就能写代码使用了。补充，今天刚发
    现一个中文的博客：[LSTM简介以及数学推导(FULL BPTT) - 天道酬勤，做一个务实的理想主义者 - 博客频道 - CSDN.NET](http://blog.csdn.net/a635661820/article/details/45390671)
    不过，稍微深入下去还是得老老实实的好好学，下面是我认为比较好的

    完整LSTM学习流程：

    :   我一直都觉得了解一个模型的前世今生对模型理解有巨大的帮助。到LSTM这里（
        假设题主零基础）那比较好的路线是MLP->RNN->LSTM。还有LSTM本身的发展路线
        （97年最原始的LSTM到forget gate到peephole再到CTC）按照这个路线学起来会
        比较顺，所以我优先推荐的两个教程都是按照这个路线来的：

        1. 多伦多大学的 Alex Graves 的RNN专著 《 Supervised Sequence Labelling with Recurrent Neural Networks 》

        2. Felix Gers的博士论文 《 Long short-term memory in recurrent neural networks 》

        这两个内容都挺多的，不过可以跳着看，反正我是没看完┑(￣Д ￣)┍

        还有一个最新的（今年2015）的综述， 《 A Critical Review of Recurrent Neural Networks for Sequence Learning 》 不过很多内容都来自以上两个材料。

        其他可以当做教程的材料还有：

        - 《 From Recurrent Neural Network to Long Short Term Memory Architecture
          Application to Handwriting Recognition Author 》
        - 《 Generating Sequences With Recurrent Neural Networks 》 （这个有对应源
          码，虽然实例用法是错的，自己用的时候还得改代码，主要是摘出一些来用，供参
          考）然后呢，可以开始编码了。除了前面提到的theano教程还有一些论文的开源代
          码，到github上搜就好了。

        顺便安利一下theano，theano的自动求导和GPU透明对新手以及学术界研究者来说非常
        方便，LSTM拓扑结构对于求导来说很复杂，上来就写LSTM反向求导还要GPU编程代码非
        常费时间的，而且搞学术不是实现一个现有模型完了，得尝试创新，改模型，每改一
        次对应求导代码的修改都挺麻烦的。

        其实到这应该算是一个阶段了，如果你想继续深入可以具体看看几篇经典论文，比如
        LSTM以及各个改进对应的经典论文。

        还有楼上提到的 《 LSTM: A Search Space Odyssey 》 通过从新进行各种实验来对
        比考查LSTM的各种改进（组件）的效果。挺有意义的，尤其是在指导如何使用LSTM方
        面。

        LSTM网络本质还是RNN网络，基于LSTM的RNN架构上的变化有最先的BRNN（双向），还
        有今年Socher他们提出的树状LSTM用于情感分析和句子相关度计算 《 Improved
        Semantic Representations From Tree-Structured Long Short-Term Memory
        Networks 》

        今年ACL（2015）上有一篇层次的LSTM 《 A Hierarchical Neural Autoencoder for
        Paragraphs and Documents 》 。使用不同的LSTM分别处理词、句子和段落级别输入
        ，并使用自动编码器（autoencoder）来检测LSTM的文档特征抽取和重建能力。

        还有一篇文章 《 Chung J, Gulcehre C, Cho K, et al.  Gated feedback recurrent neural networks[J]. arXiv preprint arXiv:1502.02367, 2015. 》，
        把gated的思想从记忆单元扩展到了网络架构上，提出多层RNN各个层的隐含层数据可以相互利用（之前的
        多层RNN多隐含层只是单向自底向上连接），不过需要设置门（gated）来调节。

        记忆单元方面，Bahdanau Dzmitry他们在构建RNN框架的机器翻译模型的时候使用了
        GRU单元（gated recurrent unit）替代LSTM，其实LSTM和GRU都可以说是gated
        hidden unit。两者效果相近，但是GRU相对LSTM来说参数更少，所以更加不容易过拟
        合。（大家堆模型堆到dropout也不管用的时候可以试试换上GRU这种参数少的

        图像处理（对，不用CNN用RNN）：

        :   《 Visin F, Kastner K, Cho K, et al. ReNet: A Recurrent Neural Network Based Alternative to Convolutional Networks[J]. arXiv preprint arXiv:1505.00393, 2015 》

            4向RNN（使用LSTM单元）替代CNN。

            使用LSTM读懂python程序：

            《 Zaremba W, Sutskever I. Learning to execute[J]. arXiv preprint arXiv:1410.4615, 2014. 》

            使用基于LSTM的深度模型用于读懂python程序并且给出
            正确的程序输出。文章的输入是短小简单python程序，这
            些程序的输出大都是简单的数字，例如0-9之内加减法程
            序。模型一个字符一个字符的输入python程序，经过多层
            LSTM后输出数字结果，准确率达到99%

        手写识别：

        :   《 Liwicki M, Graves A, Bunke H, et al. A novel approach
            to on-line handwriting recognition based on bidirectional long short-term memory 》

        机器翻译

        对话生成

        句法分析

        信息检索

        图文转换

    <hr>

    首先，对于没有RNN基础的同学，强烈建议先看一下下面这篇论文:

    A Critical Review of Recurrent Neural Networks for Sequence Learning

    里面的数学符号定义清楚，非常适合没有任何基础的童鞋对RNN和LSTM建立一个基本的
    认识。然后，看完这篇论文以后，可以接着看下面这篇博客:

    [Understanding LSTM Networks -- colah's blog](http://colah.github.io/posts/2015-08-Understanding-LSTMs/)

    里面对LSTM结构为什么这样设计，做了一步步的推理解释，非常的详细。看完上面两
    个tutorial, 你对LSTM的结构已经基本了解了。如果希望对于如何训练LSTM, 了解
    BPTT算法的工作细节，可以看Alex Graves的论文:

    Supervised Sequence Labelling with Recurrent Neural Networks

    这篇论文里有比较详细的公式推导，但是对于LSTM的结构却讲的比较混乱，所以不建
    议入门就看这篇论文。看完了上面篇论文／教程以后，对于LSTM的理论知识就基本掌
    握了，下面就需要在实践中进一步加深理解，我还没有去实践，后面的答案等实践完
    以后回来再补上。不过根据有经验的学长介绍，使用Theano自己实现一遍LSTM是一个
    不错的选择

    [The Unreasonable Effectiveness of Recurrent Neural Networks](http://karpathy.github.io/2015/05/21/rnn-effectiveness/)

[Facebook 的人工智能实验室 (FAIR) 有哪些厉害的大牛和技术积累？ - 田渊栋的回答 - 知乎](https://www.zhihu.com/question/30924352/answer/50176654)

:   人员方面，Yann LeCun毫无疑问是整个组的Director，其它大牛有VC维和SVM的缔造者
    Vladimir Vapnik，提出随机梯度下降法的Léon Bottou，做出高性能PHP虚拟机HHVM的
    Keith Adams, Rob Fergus, Jason Weston, Marc'Aurelio Ranzato, Tomas Mikolov,
    Florent Perronnin, Piotr Dollar, Hervé Jégou, Ronan Collobert, Yaniv
    Taigman等。在深度学习的时代，研究和工程已经有融合的趋势，因此FAIR这两方面的
    大牛都有。工作气氛上来说，组内较平等，讨论自由，基本没有传统的上下级观念。
    若是任何人有有趣的想法，大家都会倾听并且作出评论。要是想法正确，Yann也会
    like。

    没有人逼着干活，但大家都在努力干活。

    要是人类能掌握自组织纳米机械的设计工具和生产流水线，那再会有一次质的飞跃。
    而自然界刚开始就朝着自组织的路子走，用的是天顶星人的科技，三羧酸循环，光合
    作用，离子泵，都在微观体系下完成，没有摩擦力没有能量耗散，效率都接近百分之
    百，这这就是为啥大脑能耗低的原因。人类的科技相比之下笨重低效，差得远。

    基本上现在深度学习这一块都没有在模拟人脑，而是自己定义最简单的（甚至在生物
    学上是错误的）目标函数和网络链接结构，给定了数据集，照着数学准则用梯度下降
    求解最优参数。不同的原则会得到不同的学习算法。为什么不用和生物学一样的模型
    ？因为生物学模型太复杂了，不如做个简单数学抽象，效果好就说明在某种程度上抓
    到了本质。不然真心是一步也动不了。

    其实几条简单原理便可以创造及其宏大的空间，比如说19路围棋规则简单，但其中变
    化无穷无尽，比宇宙的基本粒子总和还多得多，在这样的空间里畅游，绝不会有重复
    之感。人工设计的规则如此，自然界更是如此，碳氢氧氮四个元素，几乎组成了全部
    有机界。

    像混沌系统如天气，微小的初值变化会对未来产生巨大影响，事实上确实是不可预测
    的。但是人脑是混沌的么？说不准。并不一定变量多的系统就混沌。洛伦兹奇怪吸引
    子只有三个变量哦，但是照样混沌；计算钢梁弯曲有无穷多个变量，照样用有限元解
    得妥妥的，为啥呀？每个系统有不同的内在结构，不可以一概而论的。

    我半年前从谷歌X的无人车组跳到Facebook的人工智能实验室（FAIR），感触良多，这
    里写一些分享给大家。

    虽然F和G并称一流的IT公司，但是其实内部是很不一样的，甚至可以说完全相反。加
    入FB之前，问过很多朋友，大家的意见综合起来是FB有点“乱”，没有统一的平台，各
    组管各组忙，代码质量比G差很多，文档也少。这听起来挺吓人的，但认真想想，反过
    来说乱才有机会。G最大的问题恰恰是一切都井然有序，能出大成果的地方都出完了，
    员工就像螺丝钉，只要在自己的岗位上做好修补就行了。

    这两天绩效考核，我老婆评论说我这半年干的事情比在G家一年还多，有产品发布也有
    研究，我想这就是真正把兴趣用在工作上的结果。我还记得自己最后一天在G的日子，
    HR小姑娘最后问我为啥离开，是不是因为X的工作太辛苦，需要一些工作和生活的平衡
    ？我笑了笑，敷衍了几句，心里想起了《冰与火之歌》里的那一句台词——

    > 雪诺，你什么也不懂。

    refs and see also

      - [关于”做人工智能是否一定要学点生物“ - 远东轶事 - 知乎专栏](http://zhuanlan.zhihu.com/p/19953128)
      - [转职半年总结 - 远东轶事 - 知乎专栏](http://zhuanlan.zhihu.com/p/20111731)

[GitHub 上有哪些有趣的关于 NLP 或者 DL 的项目？ - 知乎](https://www.zhihu.com/question/36853910)

[机器学习该怎么入门？ - 知乎](https://www.zhihu.com/question/20691338)

[机器学习经典论文/survey合集 - 机器学习 - 算法组](http://suanfazu.com/t/ji-qi-xue-xi-jing-dian-lun-wen-slash-surveyhe-ji/14)

[机器学习经典书籍 - 机器学习 - 算法组](http://suanfazu.com/t/topic/15)

[机器学习入门资源不完全汇总 | 好东西传送门出品](http://ml.memect.com/article/machine-learning-guide.html)

[机器学习 - 标签 - tornadomeet - 博客园](http://www.cnblogs.com/tornadomeet/tag/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/)

---

refs and see also

  - [Best | GitXiv](http://gitxiv.com/best)
  - [机器学习 - 话题精华 - 知乎](https://www.zhihu.com/topic/19559450/top-answers)
  - [深度学习（Deep Learning） - 话题精华 - 知乎](https://www.zhihu.com/topic/19813032/top-answers?page=1)
  - [神经网络 - 话题精华 - 知乎](https://www.zhihu.com/topic/19607065/top-answers)

---

```
// euclidean distance
distance d -> [0, inf], 1/(d+1) -> [0, 1]

// pearson correlation score
best-fit line
similarity metric: sim_pearson, sim_distance
```

[Metric (mathematics) - Wikipedia, the free encyclopedia](https://en.wikipedia.org/wiki/Metric_(mathematics)#Examples)

[Pearson product-moment correlation coefficient - Wikipedia, the free encyclopedia](https://en.wikipedia.org/wiki/Pearson_product-moment_correlation_coefficient)

<http://whudoc.qiniudn.com/2016/notepad++.7z>

---

[cataska/programming-collective-intelligence-code: Examples from Programming Collective Intelligence](https://github.com/cataska/programming-collective-intelligence-code)

[集体智慧编程 (豆瓣)](https://book.douban.com/subject/3288908/)

[Programming Collective Intelligence - O'Reilly Media](http://shop.oreilly.com/product/9780596529321.do)

[Programming Collective Intelligence (豆瓣)](https://book.douban.com/subject/2209702/)

- simulated annealing（模拟退火）
- genetic algorithms（遗传算法）
    + population（种群），hand-designed or 随机的；user-defined task
    + elitism（精英选拔）
    + mutation（变异）
    + crossover/breeding
    + 优胜劣汰的 evolutionary pressure：survival of the fittest
    + a round-robin tournament
    + evaluating trees
- massand-spring algorithms（质点弹簧算法）
- flipping around（调换求解）
- decision trees
    + CART（classification and regression trees）
    + pruning the tree（剪枝）
- kNN: k-Nearest Neighbors
    + weighted kNN
    + cross validation（交叉验证）
- kernel methods
    + kernel trick: radial-basis function （径向基函数）
- SVM
    + mamimum-margin hyperplane
    + 位于分割线：支持向量
    + 找到支持向量的算法：支持向量机
- data intensive（数据量大）
- libsvm
- bayesian classification
- trading volume（成交量）
- Non-negative Matrix Factorization (NFM)
- tanimoto coefficient
- gini impurity
- entropy

---

---

[Data Mining (豆瓣)](https://book.douban.com/subject/4888560/)

[Data Mining (豆瓣)](https://book.douban.com/subject/6533777/)
